{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a poetry model. Then, the trained model is used to complete new poems.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages. tensor2tensor will give us the Transformer model. Project Gutenberg gives us access to historical poems.\n",
    "\n",
    "\n",
    "<b>p.s.</b> Note that this notebook uses Python2 because Project Gutenberg relies on BSD-DB which was deprecated in Python 3 and removed from the standard library.\n",
    "tensor2tensor itself can be used on Python 3. It's just Project Gutenberg that has this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter-tensorboard==0.1.10\n",
      "mesh-tensorflow==0.0.5\n",
      "tensor2tensor==1.14.0\n",
      "tensorboard==1.14.0\n",
      "tensorflow==1.14.0\n",
      "tensorflow-datasets==1.2.0\n",
      "tensorflow-estimator==1.14.0\n",
      "tensorflow-gan==1.0.0.dev0\n",
      "tensorflow-hub==0.6.0\n",
      "tensorflow-metadata==0.14.0\n",
      "tensorflow-probability==0.7.0rc0\n",
      "tensorflow-serving-api==1.13.0rc1\n",
      "tensorflow-transform==0.14.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor2tensor==1.13.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/ed/67388ce4473c67d52f26955737c498515905bb308e5931e1ef4d40db3a73/tensor2tensor-1.13.1-py2.py3-none-any.whl (1.3MB)\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/ea/ab2c8c0e81bd051cc1180b104c75a865ab0fc66c89be992c4b20bbf6d624/tensorflow-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl (92.5MB)\n",
      "Collecting tensorflow-serving-api==1.13\n",
      "  Downloading https://files.pythonhosted.org/packages/63/42/b54266b031a9ebff42ba7ba2bb76f54c2cee49810f4810d81ff3c092c014/tensorflow_serving_api-1.13.0-py2.py3-none-any.whl\n",
      "Collecting gutenberg\n",
      "  Downloading https://files.pythonhosted.org/packages/73/55/1f3065df8299ebaf5df50ca70da885bf0c9f7e358a3b4166b4aeb9d7ced7/Gutenberg-0.8.0.tar.gz\n",
      "Collecting numpy (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl (17.0MB)\n",
      "Collecting six (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow-datasets (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/11/19ae6ae94f334d8c44174d9041b1e2f0096a25c8a2b797787f4f49d8c4ad/tensorflow_datasets-1.2.0-py2-none-any.whl (2.3MB)\n",
      "Collecting requests (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Collecting oauth2client (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/a9/4f25a14d23f0786b64875b91784607c2277eff25d48f915e39ff0cff505a/oauth2client-4.1.3-py2.py3-none-any.whl (98kB)\n",
      "Collecting h5py (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/90/3216b8f6d69905a320352a9ca6802a8e39fdb1cd93133c3d4163db8d5f19/h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8MB)\n",
      "Collecting gevent (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/e9/3a693414f706e12abe60554cd73c5ae8f848b182ae58018f93d86c9eb418/gevent-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl (5.0MB)\n",
      "Collecting opencv-python (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/c8/37b2c4520fcb24e68c1b2a9e20dee679ac813bc6750777aa5ba064c8056f/opencv_python-4.1.1.26-cp27-cp27mu-manylinux1_x86_64.whl (28.7MB)\n",
      "Collecting gin-config (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/43/86fd909161d2a353242756090b8e024bb07e4a9758b6863c40c6d8c48a8e/gin_config-0.2.1-py2-none-any.whl (42kB)\n",
      "Collecting future (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
      "Collecting gym (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/75/9e841bc2bc75128e0b65c3d5255d0bd16becb9d8f7120b965d41b8e70041/gym-0.14.0.tar.gz (1.6MB)\n",
      "Collecting sympy (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/21/f4105795ca7f35c541d82c5b06be684dd2f5cb4f508fb487cd7aea4de776/sympy-1.4-py2.py3-none-any.whl (5.3MB)\n",
      "Collecting kfac (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/01/f0/4a7758f854a15b37d322827123ce58619d0f4270dd94f2dd30328f397339/kfac-0.2.0-py2.py3-none-any.whl (178kB)\n",
      "Collecting gunicorn (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\n",
      "Collecting dopamine-rl (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/a6/bac332fbe7ee4b0a1abae8359f0a615265a27c2f482b802132baa2d3d636/dopamine_rl-2.0.5-py2-none-any.whl (77kB)\n",
      "Collecting mesh-tensorflow (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/f3/07fe1c894490156d0d423b95fc375fcf09371413234b94500bfcd61ab6d7/mesh_tensorflow-0.0.5-py2.py3-none-any.whl (95kB)\n",
      "Collecting pypng (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
      "Collecting jax (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/d2/869d0e5ffe8db1b9d6c93ad1761b22e2eee76383e6c96e68e7a1276ecfab/jax-0.1.46.tar.gz (219kB)\n",
      "Collecting tensorflow-probability (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
      "Collecting jaxlib (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/f2/cfee096a7de6ebdd3c721ad8beb7e5148ec00517eac7e138b48dfe93b918/jaxlib-0.1.23-cp27-none-manylinux1_x86_64.whl (24.1MB)\n",
      "Collecting scipy (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/f6/7c16d60aeb3694e5611976cb4f1eaf1c6b7f1e7c55771d691013405a02ea/scipy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl (24.8MB)\n",
      "Collecting tqdm (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB)\n",
      "Collecting google-api-python-client (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/c7/16ca16d28f2d71c8bd6fa67c91eb2a82259dc589c0504f903b675ecdaa84/google_api_python_client-1.7.11-py2-none-any.whl (55kB)\n",
      "Collecting bz2file (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Collecting flask (from tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
      "Collecting mock>=2.0.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/c3/65db90ec27181edf491c26aa998ae631e50cd1f04ee8d8d513a95e3937f3/grpcio-1.23.0-cp27-cp27mu-manylinux1_x86_64.whl (2.2MB)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/0d/7cbf64cac3f93617a2b6b079c0182e4a83a3e7a8964d3b0cc3d9758ba002/absl-py-0.8.0.tar.gz (102kB)\n",
      "Collecting wheel (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/ac/48dd71c2bdc8d31e367f9b72f25ccb3b89bc6b9d664fee21f9a8efa5714d/tensorboard-1.13.1-py2-none-any.whl (3.2MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/98/de/2ac9a18d675f5537f24e70de890352ad5f34324553cedc55073414d61f6d/protobuf-3.9.2-cp27-cp27mu-manylinux1_x86_64.whl (1.2MB)\n",
      "Collecting enum34>=1.1.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting SPARQLWrapper>=1.8.2 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/ec/515152e9bdec175131ee6d6af3ef2ba8060da4d8115d30534fc49e4cda10/SPARQLWrapper-1.8.4-py2-none-any.whl\n",
      "Collecting functools32>=3.2.3-2 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz\n",
      "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/76/f4613574cc557262861dca47e2c5fc8f2435f75424acaa73f8517aed1346/rdflib-sqlalchemy-0.3.8.tar.gz (1.2MB)\n",
      "Collecting rdflib>=4.2.0 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/77/1fa0f4cffd5faad496b1344ab665902bb2609f56e0fb19bcf80cff485da0/rdflib-4.2.2.tar.gz (905kB)\n",
      "Collecting setuptools>=18.5 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl (576kB)\n",
      "Collecting attrs (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/96/d828354fa2dbdf216eaa7b7de0db692f12c234f7ef888cc14980ef40d1d2/attrs-19.1.0-py2.py3-none-any.whl\n",
      "Collecting psutil (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/ca/5b8c1fe032a458c2c4bcbe509d1401dca9dda35c7fc46b36bb81c2834740/psutil-5.6.3.tar.gz (435kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/79/bfb672fc4ad09297f61734992c2658f753fc3d508701c5b5c47390de0ee2/tensorflow_metadata-0.14.0-py2.py3-none-any.whl\n",
      "Collecting wrapt (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting dill (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
      "Collecting promise (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/97/5fc9f9a723018bc5c55b92d7575d8a35617ca8cae070081f1d7f7fc9c935/promise-2.2.1-py2-none-any.whl\n",
      "Collecting futures (from tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\n",
      "Collecting rsa>=3.1.4 (from oauth2client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting httplib2>=0.9.1 (from oauth2client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/23/bb9606e87a66fd8c72a2b1a75b049d3859a122bc2648915be845bc44e04f/httplib2-0.13.1.tar.gz (219kB)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/be/70/e5ea8afd6d08a4b99ebfc77bd1845248d56cfcf43d11f9dc324b9580a35c/pyasn1_modules-0.2.6-py2.py3-none-any.whl (95kB)\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/6e/f2d25875713ad0885c8d3c69269697406652e6f64e1a6bd8264f7a609327/greenlet-0.4.15-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pyglet<=1.3.2,>=1.2.0 (from gym->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "Collecting cloudpickle~=1.2.0 (from gym->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Collecting mpmath>=0.19 (from sympy->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/63/3384ebb3b51af9610086b23ea976e6d27d6d97bf140a76a365bd77a3eb32/mpmath-1.1.0.tar.gz (512kB)\n",
      "Collecting opt_einsum (from jax->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
      "Collecting fastcache (from jax->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/a3/b280cba4b4abfe5f5bdc643e6c9d81bf3b9dc2148a11e5df06b6ba85a560/fastcache-1.1.0.tar.gz\n",
      "Collecting decorator (from tensorflow-probability->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.4.1 (from google-api-python-client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/9b/ed0516cc1f7609fb0217e3057ff4f0f9f3e3ce79a369c6af4a6c5ca25664/google_auth-1.6.3-py2.py3-none-any.whl (73kB)\n",
      "Collecting google-auth-httplib2>=0.0.3 (from google-api-python-client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/25/66a49231b44409d7f07cfcf2506a8b070ce3c99fc47cc256bea833f24791/uritemplate-3.0.0-py2-none-any.whl\n",
      "Collecting Werkzeug>=0.15 (from flask->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "Collecting itsdangerous>=0.24 (from flask->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Collecting click>=5.1 (from flask->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "Collecting Jinja2>=2.10.1 (from flask->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/e7/fd8b501e7a6dfe492a433deb7b9d833d39ca74916fa8bc63dd1a4947a671/Jinja2-2.10.1-py2.py3-none-any.whl (124kB)\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Collecting alembic>=0.8.8 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/42/48447bf41287bc577e4f340e7c28578e322567f5622a915bdfa01c83dc76/alembic-1.2.1.tar.gz (1.1MB)\n",
      "Collecting SQLAlchemy>=1.1.4 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/49/82d64d705ced344ba458197dadab30cfa745f9650ee22260ac2b275d288c/SQLAlchemy-1.3.8.tar.gz (5.9MB)\n",
      "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "Collecting pyparsing (from rdflib>=4.2.0->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl (65kB)\n",
      "Collecting googleapis-common-protos (from tensorflow-metadata->tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/ee/e59e74ecac678a14d6abefb9054f0bbcb318a6452a30df3776f133886d7d/googleapis-common-protos-1.6.0.tar.gz\n",
      "Collecting typing>=3.6.4; python_version < \"3.5\" (from promise->tensorflow-datasets->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/30/64ca29543375759dc589ade14a6cd36382abf2bec17d67de8481bc9814d7/typing-3.7.4.1-py2-none-any.whl\n",
      "Collecting cachetools>=2.0.0 (from google-auth>=1.4.1->google-api-python-client->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask->tensor2tensor==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/40/f3adb7cf24a8012813c5edb20329eb22d5d8e2a0ecf73d21d6b85865da11/MarkupSafe-1.1.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting Mako (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
      "Collecting python-editor>=0.3 (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/55/a0/3c0ba1c10f2ca381645dd46cb7afbb73fddc8de9f957e1f9e726a846eabc/python_editor-1.0.4-py2-none-any.whl\n",
      "Collecting python-dateutil (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
      "Building wheels for collected packages: gutenberg, future, gym, pypng, jax, bz2file, keras-applications, termcolor, absl-py, gast, functools32, rdflib-sqlalchemy, rdflib, psutil, wrapt, dill, httplib2, mpmath, opt-einsum, fastcache, alembic, SQLAlchemy, googleapis-common-protos, Mako\n",
      "  Running setup.py bdist_wheel for gutenberg: started\n",
      "  Running setup.py bdist_wheel for gutenberg: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5a/f8/ec/f067817f99eca8a42bb4253c1226f5de4e0c443445b686c85d\n",
      "  Running setup.py bdist_wheel for future: started\n",
      "  Running setup.py bdist_wheel for future: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "  Running setup.py bdist_wheel for gym: started\n",
      "  Running setup.py bdist_wheel for gym: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/7e/53/f6/c0cd3c9bf953f35c0aee7fa62ea209371e92f5e5cced3245ba\n",
      "  Running setup.py bdist_wheel for pypng: started\n",
      "  Running setup.py bdist_wheel for pypng: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
      "  Running setup.py bdist_wheel for jax: started\n",
      "  Running setup.py bdist_wheel for jax: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/c1/fc/33/0930c2c51ccec02134c74baecef99a561e622de90032bd3301\n",
      "  Running setup.py bdist_wheel for bz2file: started\n",
      "  Running setup.py bdist_wheel for bz2file: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "  Running setup.py bdist_wheel for keras-applications: started\n",
      "  Running setup.py bdist_wheel for keras-applications: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
      "  Running setup.py bdist_wheel for termcolor: started\n",
      "  Running setup.py bdist_wheel for termcolor: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py: started\n",
      "  Running setup.py bdist_wheel for absl-py: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/9a/1e/7a/456008eb5e47fd5de792c6139df6d5b3d5f71d51c6a0b94799\n",
      "  Running setup.py bdist_wheel for gast: started\n",
      "  Running setup.py bdist_wheel for gast: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Running setup.py bdist_wheel for functools32: started\n",
      "  Running setup.py bdist_wheel for functools32: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/b5/18/32/77a1030457155606ba5e3ec3a8a57132b1a04b1c4f765177b2\n",
      "  Running setup.py bdist_wheel for rdflib-sqlalchemy: started\n",
      "  Running setup.py bdist_wheel for rdflib-sqlalchemy: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/8c/7f/e5/2cfedb41ce9149b8af8c35a063b4fd1b8c1573c6391bbd5b53\n",
      "  Running setup.py bdist_wheel for rdflib: started\n",
      "  Running setup.py bdist_wheel for rdflib: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/8d/f6/b7/f5e9501d0f006fc9fd497c930206952856b2191ab5c836cb97\n",
      "  Running setup.py bdist_wheel for psutil: started\n",
      "  Running setup.py bdist_wheel for psutil: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/90/7e/74/bb640d77775e6b6a78bcc3120f9fea4d2a28b2706de1cff37d\n",
      "  Running setup.py bdist_wheel for wrapt: started\n",
      "  Running setup.py bdist_wheel for wrapt: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Running setup.py bdist_wheel for dill: started\n",
      "  Running setup.py bdist_wheel for dill: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/f4/82/e5/32e48c9506525103a5ea623ada8452739e56b4ec4d71708827\n",
      "  Running setup.py bdist_wheel for mpmath: started\n",
      "  Running setup.py bdist_wheel for mpmath: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/63/9d/8e/37c3f6506ed3f152733a699e92d8e0c9f5e5f01dea262f80ad\n",
      "  Running setup.py bdist_wheel for opt-einsum: started\n",
      "  Running setup.py bdist_wheel for opt-einsum: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
      "  Running setup.py bdist_wheel for fastcache: started\n",
      "  Running setup.py bdist_wheel for fastcache: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/6a/80/bf/30024738b03fa5aa521e2a2ac952a8d77d0c65e68d92bcd3b6\n",
      "  Running setup.py bdist_wheel for alembic: started\n",
      "  Running setup.py bdist_wheel for alembic: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/c6/b8/fd/1f16371156a8184172c4935cbbef6a345d57dd447e31a36633\n",
      "  Running setup.py bdist_wheel for SQLAlchemy: started\n",
      "  Running setup.py bdist_wheel for SQLAlchemy: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/97/b6/66/de2064d40c920adc2984ff3b8fd4f11494c8ab9e48ba87e8a2\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos: started\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/9e/3d/a2/1bec8bb7db80ab3216dbc33092bb7ccd0debfb8ba42b5668d5\n",
      "  Running setup.py bdist_wheel for Mako: started\n",
      "  Running setup.py bdist_wheel for Mako: finished with status 'done'\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
      "Successfully built gutenberg future gym pypng jax bz2file keras-applications termcolor absl-py gast functools32 rdflib-sqlalchemy rdflib psutil wrapt dill httplib2 mpmath opt-einsum fastcache alembic SQLAlchemy googleapis-common-protos Mako\n",
      "Installing collected packages: numpy, six, attrs, functools32, psutil, future, termcolor, idna, chardet, certifi, urllib3, requests, setuptools, protobuf, googleapis-common-protos, tensorflow-metadata, bz2file, tqdm, enum34, wrapt, dill, typing, promise, futures, absl-py, tensorflow-datasets, pyasn1, rsa, httplib2, pyasn1-modules, oauth2client, h5py, greenlet, gevent, opencv-python, gin-config, scipy, pyglet, cloudpickle, gym, mpmath, sympy, decorator, tensorflow-probability, kfac, gunicorn, dopamine-rl, mesh-tensorflow, pypng, opt-einsum, fastcache, jax, jaxlib, cachetools, google-auth, google-auth-httplib2, uritemplate, google-api-python-client, Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask, tensor2tensor, keras-applications, funcsigs, mock, grpcio, tensorflow-estimator, backports.weakref, wheel, markdown, tensorboard, gast, keras-preprocessing, astor, tensorflow, tensorflow-serving-api, isodate, pyparsing, rdflib, SPARQLWrapper, SQLAlchemy, Mako, python-editor, python-dateutil, alembic, rdflib-sqlalchemy, gutenberg\n",
      "Successfully installed Jinja2-2.10.1 Mako-1.1.0 MarkupSafe-1.1.1 SPARQLWrapper-1.8.4 SQLAlchemy-1.3.8 Werkzeug-0.16.0 absl-py-0.8.0 alembic-1.2.1 astor-0.8.0 attrs-19.1.0 backports.weakref-1.0.post1 bz2file-0.98 cachetools-3.1.1 certifi-2019.9.11 chardet-3.0.4 click-7.0 cloudpickle-1.2.2 decorator-4.4.0 dill-0.3.0 dopamine-rl-2.0.5 enum34-1.1.6 fastcache-1.1.0 flask-1.1.1 funcsigs-1.0.2 functools32-3.2.3.post2 future-0.17.1 futures-3.3.0 gast-0.3.2 gevent-1.4.0 gin-config-0.2.1 google-api-python-client-1.7.11 google-auth-1.6.3 google-auth-httplib2-0.0.3 googleapis-common-protos-1.6.0 greenlet-0.4.15 grpcio-1.23.0 gunicorn-19.9.0 gutenberg-0.8.0 gym-0.14.0 h5py-2.10.0 httplib2-0.13.1 idna-2.8 isodate-0.6.0 itsdangerous-1.1.0 jax-0.1.46 jaxlib-0.1.23 keras-applications-1.0.8 keras-preprocessing-1.1.0 kfac-0.2.0 markdown-3.1.1 mesh-tensorflow-0.0.5 mock-3.0.5 mpmath-1.1.0 numpy-1.16.5 oauth2client-4.1.3 opencv-python-4.1.1.26 opt-einsum-2.3.2 promise-2.2.1 protobuf-3.9.2 psutil-5.6.3 pyasn1-0.4.7 pyasn1-modules-0.2.6 pyglet-1.3.2 pyparsing-2.4.2 pypng-0.0.20 python-dateutil-2.8.0 python-editor-1.0.4 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8 requests-2.22.0 rsa-4.0 scipy-1.2.2 setuptools-41.2.0 six-1.12.0 sympy-1.4 tensor2tensor-1.13.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-datasets-1.2.0 tensorflow-estimator-1.13.0 tensorflow-metadata-0.14.0 tensorflow-probability-0.7.0 tensorflow-serving-api-1.13.0 termcolor-1.1.0 tqdm-4.36.1 typing-3.7.4.1 uritemplate-3.0.0 urllib3-1.25.6 wheel-0.33.6 wrapt-1.11.2\n",
      "Collecting tensorflow_hub\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/64/3bba86ca49ef21a4add11a4d37e3f6cd05d2e61d207ebe26a8a96b340826/tensorflow_hub-0.6.0-py2.py3-none-any.whl (84kB)\n",
      "Collecting six>=1.10.0 (from tensorflow_hub)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.12.0 (from tensorflow_hub)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting protobuf>=3.4.0 (from tensorflow_hub)\n",
      "  Using cached https://files.pythonhosted.org/packages/98/de/2ac9a18d675f5537f24e70de890352ad5f34324553cedc55073414d61f6d/protobuf-3.9.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting setuptools (from protobuf>=3.4.0->tensorflow_hub)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, numpy, setuptools, protobuf, tensorflow-hub\n",
      "Successfully installed numpy-1.16.5 protobuf-3.9.2 setuptools-41.2.0 six-1.12.0 tensorflow-hub-0.6.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install tensor2tensor==1.13.1 tensorflow==1.13.1 tensorflow-serving-api==1.13 gutenberg \n",
    "pip install tensorflow_hub \n",
    "\n",
    "# install from sou\n",
    "#git clone https://github.com/tensorflow/tensor2tensor.git\n",
    "#cd tensor2tensor\n",
    "#yes | pip install --user -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following cell does not reflect the version of tensorflow and tensor2tensor that you just installed, click **\"Reset Session\"** on the notebook so that the Python environment picks up the new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter-tensorboard==0.1.10\n",
      "mesh-tensorflow==0.0.5\n",
      "tensor2tensor==1.13.1\n",
      "tensorboard==1.13.1\n",
      "tensorflow==1.13.1\n",
      "tensorflow-datasets==1.2.0\n",
      "tensorflow-estimator==1.13.0\n",
      "tensorflow-gan==1.0.0.dev0\n",
      "tensorflow-hub==0.6.0\n",
      "tensorflow-metadata==0.14.0\n",
      "tensorflow-probability==0.7.0\n",
      "tensorflow-serving-api==1.13.0\n",
      "tensorflow-transform==0.14.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-a038efcc0f203235' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-a038efcc0f203235' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM= 'poetry_line_problem'\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PROBLEM'] = PROBLEM\n",
    "\n",
    "#os.environ['PATH'] = os.environ['PATH'] + ':' + os.getcwd() + '/tensor2tensor/tensor2tensor/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We will get some <a href=\"https://www.gutenberg.org/wiki/Poetry_(Bookshelf)\">poetry anthologies</a> from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf data/poetry\n",
    "mkdir -p data/poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gutenberg.acquire",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4a311fc17761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_etext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstrip_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m books = [\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gutenberg.acquire"
     ]
    }
   ],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import re\n",
    "\n",
    "books = [\n",
    "  # bookid, skip N lines\n",
    "  (26715, 1000, 'Victorian songs'),\n",
    "  (30235, 580, 'Baldwin collection'),\n",
    "  (35402, 710, 'Swinburne collection'),\n",
    "  (574, 15, 'Blake'),\n",
    "  (1304, 172, 'Bulchevys collection'),\n",
    "  (19221, 223, 'Palgrave-Pearse collection'),\n",
    "  (15553, 522, 'Knowles collection') \n",
    "]\n",
    "\n",
    "with open('data/poetry/raw.txt', 'w') as ofp:\n",
    "  lineno = 0\n",
    "  for (id_nr, toskip, title) in books:\n",
    "    startline = lineno\n",
    "    text = strip_headers(load_etext(id_nr)).strip()\n",
    "    lines = text.split('\\n')[toskip:]\n",
    "    # any line that is all upper case is a title or author name\n",
    "    # also don't want any lines with years (numbers)\n",
    "    for line in lines:\n",
    "      if (len(line) > 0 \n",
    "          and line.upper() != line \n",
    "          and not re.match('.*[0-9]+.*', line)\n",
    "          and len(line) < 50\n",
    "         ):\n",
    "        cleaned = re.sub('[^a-z\\'\\-]+', ' ', line.strip().lower())\n",
    "        ofp.write(cleaned)\n",
    "        ofp.write('\\n')\n",
    "        lineno = lineno + 1\n",
    "      else:\n",
    "        ofp.write('\\n')\n",
    "    print('Wrote lines {} to {} from {}'.format(startline, lineno, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: 'data/poetry/*.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "We are going to train a machine learning model to write poetry given a starting point. We'll give it one line, and it is going to tell us the next line.  So, naturally, we will train it on real poetry. Our feature will be a line of a poem and the label will be next line of that poem.\n",
    "<p>\n",
    "Our training dataset will consist of two files.  The first file will consist of the input lines of poetry and the other file will consist of the corresponding output lines, one output line per input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/poetry/raw.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-22510652aff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/poetry/raw.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrawfp\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/poetry/input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfp\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/poetry/output.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprev_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcurr_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrawfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcurr_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/poetry/raw.txt'"
     ]
    }
   ],
   "source": [
    "with open('data/poetry/raw.txt', 'r') as rawfp,\\\n",
    "  open('data/poetry/input.txt', 'w') as infp,\\\n",
    "  open('data/poetry/output.txt', 'w') as outfp:\n",
    "    \n",
    "    prev_line = ''\n",
    "    for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            infp.write(prev_line + '\\n')\n",
    "            outfp.write(curr_line + '\\n')\n",
    "        prev_line = curr_line      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'data/poetry/*.txt' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to generate the data beforehand -- instead, we can have Tensor2Tensor create the training dataset for us. So, in the code below, I will use only data/poetry/raw.txt -- obviously, this allows us to productionize our model better.  Simply keep collecting raw data and generate the training/test data at the time of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf poetry\n",
    "mkdir -p poetry/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(text_problems.Text2TextProblem):\n",
    "  \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('data/poetry/raw.txt', 'r') as rawfp:\n",
    "      prev_line = ''\n",
    "      for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            yield {\n",
    "                \"inputs\": prev_line,\n",
    "                \"targets\": curr_line\n",
    "            }\n",
    "        prev_line = curr_line          \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_poetry_tpu():\n",
    "  hparams = transformer_poetry()\n",
    "  transformer.update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_poetry_range(rhp):\n",
    "  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='poetry',\n",
    "    version='0.1',\n",
    "    author = 'Google',\n",
    "    author_email = 'training-feedback@cloud.google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Poetry Line Problem',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch poetry/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data \n",
    "\n",
    "Our problem (translation) requires the creation of text sequences from the training dataset.  This is done using t2t-datagen and the Problem defined in the previous section.\n",
    "\n",
    "(Ignore any runtime warnings about np.float64. they are harmless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see the files that were output. If you see a broken pipe error, please ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls t2t_data | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Cloud ML Engine access to data\n",
    "\n",
    "Copy the data to Google Cloud Storage, and then provide access to the data. `gsutil` throws an error when removing an empty bucket, so you may see an error the first time this code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "gsutil -m rm -r gs://${BUCKET}/poetry/\n",
    "gsutil -m cp ${DATA_DIR}/${PROBLEM}* ${DATA_DIR}/vocab* gs://${BUCKET}/poetry/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print(response['serviceAccount'])\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model locally on subset of data\n",
    "\n",
    "Let's run it locally on a subset of the data to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "BASE=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/subset\n",
    "gsutil -m rm -r $OUTDIR\n",
    "gsutil -m cp \\\n",
    "    ${BASE}/${PROBLEM}-train-0008* \\\n",
    "    ${BASE}/${PROBLEM}-dev-00000*  \\\n",
    "    ${BASE}/vocab* \\\n",
    "    $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following will work only if you are running Jupyter on a reasonably powerful machine. Don't be alarmed if your process is killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=gs://${BUCKET}/poetry/subset\n",
    "OUTDIR=./trained_model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR --job-dir=$OUTDIR --train_steps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Train model locally on full dataset (use if running on Notebook Instance with a GPU)\n",
    "\n",
    "You can train on the full dataset if you are on a Google Cloud Notebook Instance with a P100 or better GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "LOCALGPU=\"--train_steps=7500 --worker_gpu=1 --hparams_set=transformer_poetry\"\n",
    "\n",
    "DATA_DIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR ${LOCALGPU}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Train on Cloud ML Engine\n",
    "\n",
    "tensor2tensor has a convenient --cloud_mlengine option to kick off the training on the managed service.\n",
    "It uses the [Python API](https://cloud.google.com/ml-engine/docs/training-jobs) mentioned in the Cloud ML Engine docs, rather than requiring you to use gcloud to submit the job.\n",
    "<p>\n",
    "Note: your project needs P100 quota in the region.\n",
    "<p>\n",
    "The echo is because t2t-trainer asks you to confirm before submitting the job to the cloud. Ignore any error about \"broken pipe\".\n",
    "If you see a message similar to this:\n",
    "<pre>\n",
    "    [... cloud_mlengine.py:392] Launched transformer_poetry_line_problem_t2t_20190323_000631. See console to track: https://console.cloud.google.com/mlengine/jobs/.\n",
    "</pre>\n",
    "then, this step has been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "GPU=\"--train_steps=7500 --cloud_mlengine --worker_gpu=1 --hparams_set=transformer_poetry\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  ${GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## CHANGE the job name (based on output above: You will see a line such as Launched transformer_poetry_line_problem_t2t_20190322_233159)\n",
    "gcloud ml-engine jobs describe transformer_poetry_line_problem_t2t_20190323_003001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job took about <b>25 minutes</b> for me and ended with these evaluation metrics:\n",
    "<pre>\n",
    "Saving dict for global step 8000: global_step = 8000, loss = 6.03338, metrics-poetry_line_problem/accuracy = 0.138544, metrics-poetry_line_problem/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/accuracy_top5 = 0.232037, metrics-poetry_line_problem/approx_bleu_score = 0.00492648, metrics-poetry_line_problem/neg_log_perplexity = -6.68994, metrics-poetry_line_problem/rouge_2_fscore = 0.00256089, metrics-poetry_line_problem/rouge_L_fscore = 0.128194\n",
    "</pre>\n",
    "Notice that accuracy_per_sequence is 0 -- Considering that we are asking the NN to be rather creative, that doesn't surprise me. Why am I looking at accuracy_per_sequence and not the other metrics? This is because it is more appropriate for problem we are solving; metrics like Bleu score are better for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Train on a directly-connected TPU\n",
    "\n",
    "If you are running on a VM connected directly to a Cloud TPU, you can run t2t-trainer directly. Unfortunately, you won't see any output from Jupyter while the program is running.\n",
    "\n",
    "Compare this command line to the one using GPU in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# use one of these\n",
    "TPU=\"--train_steps=7500 --use_tpu=True --cloud_tpu_name=laktpu --hparams_set=transformer_poetry_tpu\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_tpu\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  ${TPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model_tpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job took about <b>10 minutes</b> for me and ended with these evaluation metrics:\n",
    "<pre>\n",
    "Saving dict for global step 8000: global_step = 8000, loss = 6.03338, metrics-poetry_line_problem/accuracy = 0.138544, metrics-poetry_line_problem/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/accuracy_top5 = 0.232037, metrics-poetry_line_problem/approx_bleu_score = 0.00492648, metrics-poetry_line_problem/neg_log_perplexity = -6.68994, metrics-poetry_line_problem/rouge_2_fscore = 0.00256089, metrics-poetry_line_problem/rouge_L_fscore = 0.128194\n",
    "</pre>\n",
    "Notice that accuracy_per_sequence is 0 -- Considering that we are asking the NN to be rather creative, that doesn't surprise me. Why am I looking at accuracy_per_sequence and not the other metrics? This is because it is more appropriate for problem we are solving; metrics like Bleu score are better for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 4: Training longer\n",
    "\n",
    "Let's train on 4 GPUs for 75,000 steps. Note the change in the last line of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "XXX This takes 3 hours on 4 GPUs. Remove this line if you are sure you want to do this.\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_full2\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --train_steps=75000 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job took <b>12 hours</b> for me and ended with these metrics:\n",
    "<pre>\n",
    "global_step = 76000, loss = 4.99763, metrics-poetry_line_problem/accuracy = 0.219792, metrics-poetry_line_problem/accuracy_per_sequence = 0.0192308, metrics-poetry_line_problem/accuracy_top5 = 0.37618, metrics-poetry_line_problem/approx_bleu_score = 0.017955, metrics-poetry_line_problem/neg_log_perplexity = -5.38725, metrics-poetry_line_problem/rouge_2_fscore = 0.0325563, metrics-poetry_line_problem/rouge_L_fscore = 0.210618\n",
    "</pre>\n",
    "At least the accuracy per sequence is no longer zero. It is now 0.0192308 ... note that we are using a relatively small dataset (12K lines) and this is *tiny* in the world of natural language problems.\n",
    "<p>\n",
    "In order that you have your expectations set correctly: a high-performing translation model needs 400-million lines of input and takes 1 whole day on a TPU pod!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model   #_modeltpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-predict\n",
    "\n",
    "How will our poetry model do when faced with Rumi's spiritual couplets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/poetry/rumi.txt\n",
    "Where did the handsome beloved go?\n",
    "I wonder, where did that tall, shapely cypress tree go?\n",
    "He spread his light among us like a candle.\n",
    "Where did he go? So strange, where did he go without me?\n",
    "All day long my heart trembles like a leaf.\n",
    "All alone at midnight, where did that beloved go?\n",
    "Go to the road, and ask any passing traveler\n",
    "That soul-stirring companion, where did he go?\n",
    "Go to the garden, and ask the gardener\n",
    "That tall, shapely rose stem, where did he go?\n",
    "Go to the rooftop, and ask the watchman\n",
    "That unique sultan, where did he go?\n",
    "Like a madman, I search in the meadows!\n",
    "That deer in the meadows, where did he go?\n",
    "My tearful eyes overflow like a river\n",
    "That pearl in the vast sea, where did he go?\n",
    "All night long, I implore both moon and Venus\n",
    "That lovely face, like a moon, where did he go?\n",
    "If he is mine, why is he with others?\n",
    "Since hes not here, to what there did he go?\n",
    "If his heart and soul are joined with God,\n",
    "And he left this realm of earth and water, where did he go?\n",
    "Tell me clearly, Shams of Tabriz,\n",
    "Of whom it is said, The sun never dieswhere did he go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the odd-numbered lines. We'll compare how close our model can get to the beauty of Rumi's second lines given his first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "awk 'NR % 2 == 1' data/poetry/rumi.txt | tr '[:upper:]' '[:lower:]' | sed \"s/[^a-z\\'-\\ ]//g\" > data/poetry/rumi_leads.txt\n",
    "head -3 data/poetry/rumi_leads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# same as the above training job ...\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model #_tpu  # or ${TOPDIR}/poetry/model_full\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry #_tpu\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note </b> if you get an error about \"AttributeError: 'HParams' object has no attribute 'problems'\" please <b>Reset Session</b>, run the cell that defines the PROBLEM and run the above cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are still phrases and not complete sentences. This indicates that we might need to train longer or better somehow. We need to diagnose the model ...\n",
    "<p>\n",
    "    \n",
    "### Diagnosing training run\n",
    "\n",
    "<p>\n",
    "Let's diagnose the training run to see what we'd improve the next time around.\n",
    "(Note that this package may not be present on Jupyter -- `pip install pydatalab` if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/poetry/model_full2'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"diagrams/poetry_loss.png\"/></td>\n",
    "<td><img src=\"diagrams/poetry_acc.png\"/></td>\n",
    "</table>\n",
    "Looking at the loss curve, it is clear that we are overfitting (note that the orange training curve is well below the blue eval curve). Both loss curves and the accuracy-per-sequence curve, which is our key evaluation measure, plateaus after 40k. (The red curve is a faster way of computing the evaluation metric, and can be ignored). So, how do we improve the model? Well, we need to reduce overfitting and make sure the eval metrics keep going down as long as the loss is also going down.\n",
    "<p>\n",
    "What we really need to do is to get more data, but if that's not an option, we could try to reduce the NN and increase the dropout regularization. We could also do hyperparameter tuning on the dropout and network sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "tensor2tensor also supports hyperparameter tuning on Cloud ML Engine. Note the addition of the autotune flags.\n",
    "<p>\n",
    "The `transformer_poetry_range` was registered in problem.py above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "XXX This takes about 15 hours and consumes about 420 ML units.  Uncomment if you wish to proceed anyway\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_hparam\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --hparams_range=transformer_poetry_range \\\n",
    "  --autotune_objective='metrics-poetry_line_problem/accuracy_per_sequence' \\\n",
    "  --autotune_maximize \\\n",
    "  --autotune_max_trials=4 \\\n",
    "  --autotune_parallel_trials=4 \\\n",
    "  --train_steps=7500 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above job, it took about 15 hours and finished with these as the best parameters:\n",
    "<pre>\n",
    "{\n",
    "      \"trialId\": \"37\",\n",
    "      \"hyperparameters\": {\n",
    "        \"hp_num_hidden_layers\": \"4\",\n",
    "        \"hp_learning_rate\": \"0.026711152525921437\",\n",
    "        \"hp_hidden_size\": \"512\",\n",
    "        \"hp_attention_dropout\": \"0.60589466163419292\"\n",
    "      },\n",
    "      \"finalMetric\": {\n",
    "        \"trainingStep\": \"8000\",\n",
    "        \"objectiveValue\": 0.0276162791997\n",
    "      }\n",
    "</pre>\n",
    "In other words, the accuracy per sequence achieved was 0.027 (as compared to 0.019 before hyperparameter tuning, so a <b>40% improvement!</b>) using 4 hidden layers, a learning rate of 0.0267, a hidden size of 512 and droput probability of 0.606. This is inspite of training for only 7500 steps instead of 75,000 steps ... we could train for 75k steps with these parameters, but I'll leave that as an exercise for you.\n",
    "<p>\n",
    "Instead, let's try predicting with this optimized model. Note the addition of the hp* flags in order to override the values hardcoded in the source code. (there is no need to specify learning rate and dropout because they are not used during inference). I am using 37 because I got the best result at trialId=37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# same as the above training job ...\n",
    "BEST_TRIAL=28  # CHANGE as needed.\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_hparam/$BEST_TRIAL\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE \\\n",
    "  --hparams=\"num_hidden_layers=4,hidden_size=512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first three line. I'm showing the first line of the couplet provided to the model, how the AI model that we trained complets it and how Rumi completes it:\n",
    "<p>\n",
    "INPUT: where did the handsome beloved go <br/>\n",
    "AI: where art thou worse to me than dead <br/>\n",
    "RUMI: I wonder, where did that tall, shapely cypress tree go?\n",
    "<p>\n",
    "INPUT: he spread his light among us like a candle <br/>\n",
    "AI: like the hurricane eclipse <br/>\n",
    "RUMI: Where did he go? So strange, where did he go without me? <br/>\n",
    "<p>\n",
    "INPUT: all day long my heart trembles like a leaf <br/>\n",
    "AI: and through their hollow aisles it plays <br/>\n",
    "RUMI: All alone at midnight, where did that beloved go? \n",
    "<p>\n",
    "Oh wow. The couplets as completed are quite decent considering that:\n",
    "* We trained the model on American poetry, so feeding it Rumi is a bit out of left field.\n",
    "* Rumi, of course, has a context and thread running through his lines while the AI (since it was fed only that one line) doesn't. \n",
    "\n",
    "<p>\n",
    "\"Spreading light like a hurricane eclipse\" is a metaphor I won't soon forget. And it was created by a machine learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving poetry\n",
    "\n",
    "How would you serve these predictions? There are two ways:\n",
    "<ol>\n",
    "<li> Use [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/deploying-models) -- this is serverless and you don't have to manage any infrastructure.\n",
    "<li> Use [Kubeflow](https://github.com/kubeflow/kubeflow/blob/master/user_guide.md) on Google Kubernetes Engine -- this uses clusters but will also work on-prem on your own Kubernetes cluster.\n",
    "</ol>\n",
    "<p>\n",
    "In either case, you need to export the model first and have TensorFlow serving serve the model. The model, however, expects to see *encoded* (i.e. preprocessed) data. So, we'll do that in the Python Flask application (in AppEngine Flex) that serves the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_full2\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-exporter \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export | tail -1)\n",
    "echo $MODEL_LOCATION\n",
    "saved_model_cli show --dir $MODEL_LOCATION --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mlengine.json\n",
    "description: Poetry service on ML Engine\n",
    "autoScaling:\n",
    "    minNodes: 1  # We don't want this model to autoscale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"poetry\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "#gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} \\\n",
    "       --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.13 --config=mlengine.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kubeflow\n",
    "\n",
    "Follow these instructions:\n",
    "* On the GCP console, launch a Google Kubernetes Engine (GKE) cluster named 'poetry' with 2 nodes, each of which is a n1-standard-2 (2 vCPUs, 7.5 GB memory) VM\n",
    "* On the GCP console, click on the Connect button for your cluster, and choose the CloudShell option\n",
    "* In CloudShell, run: \n",
    "    ```\n",
    "    git clone https://github.com/GoogleCloudPlatform/training-data-analyst`\n",
    "    cd training-data-analyst/courses/machine_learning/deepdive/09_sequence\n",
    "    ```\n",
    "* Look at [`./setup_kubeflow.sh`](setup_kubeflow.sh) and modify as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AppEngine\n",
    "\n",
    "What's deployed in Cloud ML Engine or Kubeflow is only the TensorFlow model. We still need a preprocessing service. That is done using AppEngine.  Edit application/app.yaml appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat application/app.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd application\n",
    "#gcloud app create  # if this is your first app\n",
    "#gcloud app deploy --quiet --stop-previous-version app.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visit https://mlpoetry-dot-cloud-training-demos.appspot.com and try out the prediction app!\n",
    "\n",
    "<img src=\"diagrams/poetry_app.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
